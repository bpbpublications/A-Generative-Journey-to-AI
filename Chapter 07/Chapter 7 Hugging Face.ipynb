{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278a230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForQuestionAnswering\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46d6230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = TFBertForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "980cbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = r\"\"\"\n",
    "The history of NLP generally starts in the 1950s, although work can be found from earlier periods. \n",
    "In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed \n",
    "what is now called the Turing Test as a criterion of intelligence.\n",
    "\"\"\"\n",
    "\n",
    "question = \"When did the history of NLP start?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663d7808",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors='tf')\n",
    "input_ids = inputs[\"input_ids\"].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a93db02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 1950s\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "start_scores, end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "# Find the tokens with the highest `start` and `end` scores\n",
    "start = tf.argmax(start_scores, axis=1).numpy()[0]\n",
    "end = tf.argmax(end_scores, axis=1).numpy()[0] + 1  # add 1 to get the inclusive end\n",
    "answer_tokens = input_ids[start:end]\n",
    "\n",
    "# Decode the tokens to a string\n",
    "answer = tokenizer.decode(answer_tokens)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fdc1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d0fcee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7492d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"The future of AI in healthcare is\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "818cc6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI in healthcare is uncertain. The future of AI in healthcare is uncertain.\n",
      "\n",
      "The future of AI in healthcare is uncertain. The future of AI in healthcare is uncertain.\n",
      "\n",
      "The future of AI in healthcare is uncertain. The future of AI in healthcare is uncertain.\n",
      "\n",
      "The future of AI in healthcare is uncertain. The future of AI in healthcare is uncertain.\n",
      "\n",
      "The future of AI in healthcare is uncertain. The future of AI in healthcare is uncertain.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1)\n",
    "\n",
    "# Decode the output to a human-readable format\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977d447b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
